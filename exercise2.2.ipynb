{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Exercise 2\n",
    "# Group 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping and normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using the builtin Keras method\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# derive a validation set from the training set\n",
    "# the original training set is split into \n",
    "# new training set (90%) and a validation set (10%)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.10, random_state=101)\n",
    "y_train, y_val = train_test_split(y_train, test_size=0.10, random_state=101)\n",
    "\n",
    "# the shape of the data matrix is NxHxW, where\n",
    "# N is the number of images,\n",
    "# H and W are the height and width of the images\n",
    "# keras expect the data to have shape NxHxWxH, where\n",
    "# C is the channel dimension\n",
    "X_train = np.reshape(X_train, (-1,28,28,1)) \n",
    "X_val = np.reshape(X_val, (-1,28,28,1))\n",
    "X_test = np.reshape(X_test, (-1,28,28,1))\n",
    "\n",
    "# convert the datatype to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize our data values to the range [0,1]\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert 1D class arrays to 10D class matrices\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_val = np_utils.to_categorical(y_val, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model. The amount of neurons can be changed and additional layers can be added below.\n",
    "For this exercise the middle dense layers can be removed to connect input directly to output. \n",
    "Removing the activation removes the non-linearity between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28,1)))   # flatten the 28x28x1 pixel input images to a row of pixels (a 1D-array)\n",
    "model.add(Dense(64, activation=None)) # fully connected layer with 64 neurons and ReLU nonlinearity\n",
    "model.add(Dense(64, activation=None)) # fully connected layer with 64 neurons and ReLU nonlinearity\n",
    "model.add(Dense(64, activation=None)) # fully connected layer with 64 neurons and ReLU nonlinearity\n",
    "model.add(Dense(10, activation='softmax'))  # output layer with 10 nodes (one for each class) and softmax nonlinearity\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) # compile the model\n",
    "model_name=\"Three layer 64 neurons no activation\"\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging the training process and training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 6s 116us/step - loss: 0.5111 - acc: 0.8549 - val_loss: 0.3675 - val_acc: 0.8947\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 6s 111us/step - loss: 0.3259 - acc: 0.9070 - val_loss: 0.3319 - val_acc: 0.9085\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 6s 108us/step - loss: 0.3043 - acc: 0.9132 - val_loss: 0.3172 - val_acc: 0.9123\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.2939 - acc: 0.9177 - val_loss: 0.3121 - val_acc: 0.9125\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 6s 104us/step - loss: 0.2858 - acc: 0.9193 - val_loss: 0.3112 - val_acc: 0.9095\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 0.2812 - acc: 0.9211 - val_loss: 0.3094 - val_acc: 0.9117\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 5s 97us/step - loss: 0.2774 - acc: 0.9221 - val_loss: 0.3003 - val_acc: 0.9163\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 5s 98us/step - loss: 0.2740 - acc: 0.9234 - val_loss: 0.3137 - val_acc: 0.9097\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 6s 107us/step - loss: 0.2718 - acc: 0.9242 - val_loss: 0.3163 - val_acc: 0.9143\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 5s 98us/step - loss: 0.2690 - acc: 0.9247 - val_loss: 0.2960 - val_acc: 0.9155\n",
      "Loss:  0.27417428636699914\n",
      "Accuracy:  0.9217\n"
     ]
    }
   ],
   "source": [
    "# create a way to monitor our model in Tensorboard\n",
    "tensorboard = TensorBoard(\"logs/\" + model_name)\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_val, y_val), callbacks=[tensorboard])\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "model.save('num_reader.model')\n",
    "\n",
    "print(\"Loss: \",score[0])\n",
    "print(\"Accuracy: \",score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model expects an image with number 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = tf.keras.models.load_model('num_reader.model')\n",
    "predictions = new.predict([X_test])\n",
    "np.argmax(predictions[55]) #return index of max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.resize(X_test[55],(28,28))\n",
    "plt.imshow(test)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
